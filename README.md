# Bengali BPE Tokenization Pipeline

This repository demonstrates **Byte Pair Encoding (BPE)** adapted for **Bengali grapheme clusters**, ensuring tokenization respects complex Unicode characters while producing efficient subword tokens.

---

##  Overview

Bengali, like many Indic scripts, contains **grapheme clusters** that represent a single "unit" for readers but are composed of multiple Unicode codepoints.  
Na√Øve BPE may split within grapheme clusters, creating invalid tokens.  
Our method introduces a **grapheme-aware constraint** to prevent merges across cluster boundaries.

---
WHY THIS PROJECT EXISTS
-----------------------
Modern LLMs like ChatGPT are trained and tokenized primarily with English in mind. 
Bengali (and many Indic scripts) use *grapheme clusters*‚Äîvisual characters that are often composed of multiple Unicode code points. 
Na√Øve byte/character tokenizers tend to split these clusters, creating more tokens for the same content and harming both compression and model understanding. 
This project demonstrates:
  ‚Ä¢ A simple UTF‚Äë8 BPE training loop on Bengali text (Jupyter-style steps), 
  ‚Ä¢ How merges can cross grapheme boundaries (and how to detect/report this), 
  ‚Ä¢ A Streamlit app (app.py) that visualizes graphemes, bytes, merges, and boundary violations, 
  ‚Ä¢ Why grapheme-aware tokenization helps models reason better about Bengali, lowering token count, and boosting context window efficiency.

## Screenshot
![Jupyter file's demo image](https://github.com/Sayan-Maity-Code/Tokenization/blob/master/Demo/jupyter%20file%20demo.png)

##  Quickstart

### Clone the Repository and run all cells for ipynb file
```bash
git clone https://github.com/Sayan-Maity-Code/Tokenization.git
cd tokenization_regional_lang.ipynb


```
## for visual representation
```bash
cd app.py
pip install streamlit plotly regex pandas numpy
streamlit run app.py
```
## üìÇ Repository Structure
.
‚îú‚îÄ‚îÄ tokenizer_notebook.ipynb   # Jupyter notebook with training & tokenization pipeline
‚îú‚îÄ‚îÄ app.py                     # Streamlit visualization app
‚îî‚îÄ‚îÄ README.txt                 # This file
```

---

## Pipeline

The tokenization process is illustrated as a flow diagram:

```mermaid
graph TD
    A[Bengali Text] --> B(Unicode NFC Normalization)
    B --> C(Grapheme Cluster Extraction)
    C --> D(UTF-8 Byte Encoding)
    D --> E(Byte Pair Merging)
    E --> F(Grapheme Boundary Check)
    F --> G{Crosses Boundary?}
    G -->|Yes| H[Reject Merge]
    G -->|No| I[Accept Merge]
    I --> J[Compressed Tokens]
    J --> K[Visualization]
```

---

## Jupyter Notebook

- Loads a sample Bengali corpus
- Normalizes Unicode text
- Extracts grapheme clusters using `regex` (`\X`)
- Trains a BPE tokenizer with boundary constraints
- Saves trained vocabulary

**Snippet:**

```python
import regex

text = "‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞‡•§"
clusters = regex.findall(r'\X', text)
print(clusters)  # Grapheme-aware segmentation
```
1) JUPYTER NOTEBOOK WALKTHROUGH (STEP BY STEP)
----------------------------------------------
The notebook logically progresses through the following stages. If you were to run this notebook end-to-end, this is the conceptual flow and the rationale behind each block.

A. Imports and Setup
   - We rely on:
       unicodedata      ‚Üí Unicode normalization (e.g., NFC)
       regex            ‚Üí For \X (grapheme cluster) extraction
       collections      ‚Üí Counter for counting pairs/frequencies
       itertools        ‚Üí pairwise for consecutive pair extraction
       (optional) pandas/numpy/plot libraries for any tabular/plot inspection
   - Set the sample Bengali text into a variable, e.g. `ben_content_to_tokenize`.
   - Normalize all text to NFC before *any* processing so code points are consistent.

B. Grapheme, Byte, and Pair Utilities
   1) `count_consecutive_pairs_of_bengali_token(x)`
      - Normalizes, splits into grapheme clusters via `regex.findall(r'\X', text)`.
      - Encodes each grapheme to UTF‚Äë8 bytes (tuples of ints) and then counts consecutive pairs of bytes using `pairwise`.
      - Returns a `Counter` over byte-pair tuples.

   2) `count_consecutive_pairs_of_bengali_text(x)`
      - Same structure but counts pairwise *graphemes* (not bytes).
      - Useful to compare byte-level vs grapheme-level structure.

C. Minimal UTF‚Äë8 BPE (Byte Pair Encoding) Trainer
   Function: `bengali_utf8_bpe(text, max_merges=10)`
   - Normalize text (NFC).
   - Split into graphemes with `\X`, then flatten all graphemes into one byte sequence (each code point ‚Üí UTF‚Äë8 bytes 0‚Äì255).
   - Iteratively:
       ‚Ä¢ Count all consecutive byte pairs in the current sequence.
       ‚Ä¢ Pick the most frequent pair; if its frequency < 2, stop.
       ‚Ä¢ Assign a new token ID starting from 256 to represent that pair.
       ‚Ä¢ Replace all occurrences of that pair with the new token.
       ‚Ä¢ Record the merge (for decoding later).
   - Print progress after each merge: chosen pair, frequency, sequence length.
   - Return: compressed sequence (list of ints) and the `merges` table mapping new IDs ‚Üí (left,right).

   Notes & Pitfalls:
   - Always decode/encode as UTF‚Äë8; mixing encodings will corrupt bytes.
   - The naive BPE here merges *bytes*, not graphemes‚Äîso tokens can straddle grapheme boundaries.
   - The original sample code had a few small typos in the final stats section. 
     For example, expressions like `len(graphemes)3` or using `\*` inside f-strings should be corrected to valid Python expressions.
     Example correction for summary stats:
       initial_bytes = sum(len(g.encode("utf-8")) for g in graphemes)
       reduction = ((initial_bytes - len(sequence)) / initial_bytes) * 100

D. UTF‚Äë8 BPE Decoder
   Function: `bpe_decode(token_sequence, merges, errors="strict")`
   - Recursively expand each token:
       ‚Ä¢ If < 256 ‚Üí it‚Äôs a raw byte
       ‚Ä¢ Else ‚Üí look up (left,right) and expand both sides.
   - Concatenate the resulting bytes and decode from UTF‚Äë8 to recover text.
   - If merges are incorrect, decoding will fail; `errors="replace"` can be used to visualize issues.

E. Sanity Check Round-Trip
   - Run `compressed_tokens, merge_table = bengali_utf8_bpe(...)`.
   - Then `original_text = bpe_decode(compressed_tokens, merge_table)`.
   - Confirm `original_text == ben_content_to_tokenize`.
   - If not equal, check for merge-table errors or accidental lossy handling of bytes.

F. Grapheme-Aware Analysis Helpers
   1) `analyze_bengali_text_structure(text)`
      - Normalizes ‚Üí graphemes via `\X`.
      - For each grapheme, store:
          index, raw string, Unicode code points & names, UTF‚Äë8 byte list, byte count,
          and whether it is ‚Äúcomplex‚Äù (composed of multiple code points).
      - Outputs: `graphemes`, `grapheme_analysis`, and a normalization-change flag.

   2) `classify_bengali_complexity(grapheme)`
      - Labels a grapheme: ‚ÄúSimple‚Äù vs ‚ÄúVowel diacritic‚Äù vs ‚ÄúConsonant conjunct‚Äù etc.
      - Uses code-point ranges (e.g., 0x09BE..0x09C4 for vowel signs) and virama 0x09CD.

   3) `bengali_utf8_bpe_with_grapheme_awareness(text, max_merges)`
      - Runs the same byte-level BPE but *records* whether the most-common pair appears inside a single grapheme‚Äôs byte span or crosses grapheme boundaries.
      - Returns: `sequence`, `merges`, `merge_history`, `initial_length`, `text_analysis`.
      - NOTE: This still performs byte merges; it just keeps visibility into grapheme alignment.
        A true grapheme-aware BPE would *forbid* merges that cross grapheme boundaries or would build merges only within grapheme byte ranges.

   4) `check_grapheme_boundary(byte_pair, grapheme_byte_sequences)`
      - Checks if a given byte-pair occurs wholly within any single grapheme‚Äôs byte encoding.
      - Useful for flagging ‚Äúunsafe‚Äù merges that split visual characters.

G. Token & Boundary Visualizers (Notebook-side)
   - `generate_grapheme_aware_visualization(text_analysis, compressed_tokens, merges)`
     Expands each token to bytes ‚Üí to text, color-codes tokens, and adds a dashed border if a token spans *multiple* graphemes.
   - `generate_grapheme_boundary_visualization(text_analysis, compressed_tokens, merge_table)`
     Shows a simple token-by-token text with uniform styling so you can visually inspect how tokens map to characters.
   - These functions return HTML strings that the Streamlit app can render; in the notebook you can also print snippets or validate logic.

H. Metrics & Diagnostics
   - Compute:
       ‚Ä¢ Total tokens in compressed sequence
       ‚Ä¢ Number (and rate) of grapheme-boundary violations
       ‚Ä¢ Compression ratio vs initial bytes
   - Inspect the top-N merge steps and whether the most frequent pairs tend to be within graphemes or not.
   - Inspect which Bengali patterns (e.g., vowel signs, virama-based conjuncts) are routinely split.


2) ASCII DIAGRAM: BYTES, GRAPHEMES, TOKENS
------------------------------------------
Below is a conceptual diagram showing how Bengali text maps from graphemes to UTF‚Äë8 bytes, and how BPE merges can either respect or split grapheme boundaries.

Text  ‚Üí  [  ‡¶ï   ‡¶ø  ] [  ‡¶∂  ‡ßÅ  ] [  ‡¶≠  ‡¶æ  ]  ...   (grapheme clusters)
           \_____ /    \_____/     \_____/

UTF‚Äë8 bytes (illustrative; not exact values):
[ E0 A6 95 | E0 A6 BF ] [ E0 A6 B6 | E0 A7 81 ] [ E0 A6 AD | E0 A6 BE ] ...

Na√Øve BPE merges on raw bytes (example):
Step 1: Merge most common pair (A,B) ‚Üí 256
Step 2: Merge most common pair (256,C) ‚Üí 257
...

If a token (e.g., 257) spans bytes across the '|' boundary above, it *splits* a grapheme cluster.
This produces dashed-border tokens in the visualization and increases violation count.

Grapheme-aware BPE (ideal) restricts merges to occur only within the byte ranges that compose a single grapheme.
This preserves visual characters as atomic tokens.

---

## Streamlit Visualization

Run the interactive visualization with:

```bash
streamlit run app.py
```

Features:
- Input Bengali text
- Visualize grapheme clusters with color coding
- Display BPE merges step-by-step
- Highlight rejected merges crossing cluster boundaries

**Snippet:**

```python
import streamlit as st

st.title("Bengali BPE Grapheme-Aware Tokenizer")
user_input = st.text_input("Enter Bengali text:")

# Tokenization + Visualization logic goes here
```

---

##  Why This Matters

-  Prevents invalid subword units
-  Preserves readability of tokens
-  Reduces vocabulary size while staying linguistically valid
-  Helps LLMs perform better on Bengali & Indic languages

---
STREAMLIT APP (app.py) OVERVIEW
----------------------------------
## Screenshot
![App.py file's demo image](https://github.com/Sayan-Maity-Code/Tokenization/blob/master/Demo/App.py%20demo.png)
Goal: Provide an interactive UI that demonstrates how Bengali grapheme clusters behave during byte‚Äëpair merges and why preserving grapheme boundaries matters.

A. Page Setup & Styling
   - `st.set_page_config(...)` for title, icon, wide layout.
   - Custom CSS for colored ‚Äúinfo", ‚Äúchallenge‚Äù, ‚Äúsuccess‚Äù boxes and for token/grapheme displays.

B. Sidebar: Educational Content
   - Explains: what grapheme clusters are, why normalization matters, and how tokenization impacts model quality.
   - Key bullets:
       ‚Ä¢ Grapheme clusters: visual unit vs multiple code points
       ‚Ä¢ NFC normalization to avoid duplicate encodings of the same character
       ‚Ä¢ Tokenization issues: splitting clusters, inefficiency, model confusion

C. Main Panel: Inputs
   - Drop-down sample texts (simple, diacritics-heavy, conjunct-rich, or long speech).
   - Multi-line text area for user input.
   - Slider for number of BPE merges (1..15).

D. Grapheme Analysis
   - Runs `analyze_bengali_text_structure(user_text)`.
   - Shows: whether normalization changed anything, a color bar of graphemes 
     (green for simple, red for complex), and a dataframe of grapheme details:
       ‚Ä¢ Grapheme, count of code points, UTF‚Äë8 byte length, complexity type, truncated Unicode names.
   - Metrics: total graphemes, count of complex clusters, total UTF‚Äë8 bytes, average bytes per grapheme.

E. BPE Tokenization Analysis
   - Runs `bengali_utf8_bpe_with_grapheme_awareness(user_text, max_merges)`.
   - Two visualizations:
       ‚Ä¢ Token-wise colored text (merged tokens colored; dashed border if token crosses grapheme boundary)
       ‚Ä¢ Boundary-aligned view (uniform token boxes for easy scanning)
   - Shows token ID sequence (truncated if long).
   - Computes and displays:
       ‚Ä¢ Total tokens
       ‚Ä¢ Grapheme boundary violations (count & %)
       ‚Ä¢ Compression percentage (from initial flat UTF‚Äë8 bytes ‚Üí compressed tokens)
   - Expanders for detailed violation list and token legend.

F. Educational Comparison & Technical Details
   - ‚ÄúProblems with Naive Tokenization‚Äù vs ‚ÄúGrapheme‚ÄëAware Solutions‚Äù.
   - Implementation notes with code snippets (NFC, \X usage, merging preferences).

G. Key Takeaways (Footer)
   - 1) Grapheme clusters are the true units for Bengali.
   - 2) Normalize before tokenizing.
   - 3) Standard BPE often breaks clusters and hurts performance.
   - 4) Specialized tokenizers improve modeling and generation quality.
   - 5) Visual tools make pitfalls obvious and reproducible.

H. Implementation Cautions
   - Do not assume 1 code point == 1 character.
   - Always analyze merges against grapheme boundaries.
   - If you want a *strict* grapheme-aware BPE, *forbid* merges that cross a grapheme‚Äôs byte boundary.
   - For production, consider trained vocabularies that maximize within‚Äëgrapheme merges and common multi‚Äëgrapheme morphemes while *never* splitting a grapheme.


4) WHY LLMs STRUGGLE WITH REGIONAL LANGUAGES (TRAINING DATA & TOKENIZATION)
---------------------------------------------------------------------------
A. Training Data Skew
   - Many public web corpora over-represent English; regional languages like Bengali are under‚Äërepresented.
   - Consequence: models show weaker fluency, poorer factual recall, and higher rates of malformed text in under‚Äërepresented scripts.

B. Tokenization Mismatch
   - English-friendly tokenizers (ASCII/Latin-biased) inflate token counts for Indic scripts.
   - More tokens ‚Üí higher inference/training cost for the *same* semantic content.
   - Splitting graphemes leads models to ‚Äúsee‚Äù fragments of characters, which have no linguistic meaning; gradient signal becomes noisy.

C. Why ‚ÄúFewer Tokens‚Äù Helps
   - Next-token prediction scales with token count. If the same text uses fewer, *more meaningful* tokens, models learn and predict better.
   - Lower token counts stretch effective context windows. With a fixed context size, fewer tokens mean you can fit longer Bengali passages, improving coherence across long documents.

D. Remedies
   - Curate larger, high-quality Bengali corpora (balanced domains).
   - Use grapheme-aware tokenizers or multilingual tokenizers trained with Indic scripts in mind.
   - Validate tokenization with diagnostics like the violation counter and boundary-aware visualizations shown here.
   - Fine-tune on Bengali tasks with evaluation suites that penalize malformed output.


5) PRACTICAL CHECKLIST FOR YOUR OWN DATA
----------------------------------------
[ ] Normalize to NFC on ingest
[ ] Extract graphemes with \X and compute basic stats
[ ] Train BPE merges but *track* and *forbid* boundary-crossing merges
[ ] Validate decode round-trips are lossless
[ ] Compare token counts vs a baseline (e.g., default GPT tokenizer)
[ ] Measure grapheme violation rate (should be 0 for strict tokenizer)
[ ] Evaluate on generation tasks for well-formedness (no broken conjuncts/diacritics)
[ ] Continuously visualize merges during training to catch regressions


6) EXTENSIONS & NEXT STEPS
--------------------------
- Strict Grapheme-Aware BPE:
  Implement a training loop that only allows merges inside a single grapheme‚Äôs byte span. 
  Optionally add a penalty or hard ban for cross-boundary merges.

- Morpheme-Aware Extensions:
  Once grapheme integrity is guaranteed, learn merges that align with common Bengali morphemes/affixes to improve semantic efficiency.

- Benchmarking:
  Build a small benchmark: token count, perplexity, and generation quality on Bengali news, literature, and social media text.

- Multilingual Sharing:
  Apply the same approach to other Indic scripts (Devanagari, Gurmukhi, Tamil, etc.) and compare violation rates and token savings.


7) FILES YOU‚ÄôLL HAVE
--------------------
- Notebook (conceptual):
  ‚Ä¢ Contains the functions described in sections 1B‚Äì1H and sanity checks.

- Streamlit app (app.py):
  ‚Ä¢ Imports the same analysis/tokenization helpers (or duplicates their logic).
  ‚Ä¢ Renders the UI and interactive visualizations described in section 3.
  ‚Ä¢ Can be run with:  `streamlit run app.py`

- (Optional) Data:
  ‚Ä¢ One or more Bengali text samples (.txt). Make sure they‚Äôre UTF‚Äë8 and normalized to NFC.


8) QUICK ‚Äúapp.py‚Äù SKELETON (PLAIN TEXT, FOR REFERENCE)
------------------------------------------------------
This is a *readable outline*, not a drop‚Äëin file. It mirrors the logic you prototyped in the notebook.

    import streamlit as st
    import unicodedata, regex
    from collections import Counter
    from itertools import pairwise
    import pandas as pd

    # --- helpers: analyze_bengali_text_structure, classify_bengali_complexity,
    #              bengali_utf8_bpe_with_grapheme_awareness,
    #              generate_grapheme_aware_visualization, generate_grapheme_boundary_visualization,
    #              bpe_decode ---
    # (Implement exactly as discussed in sections 1F‚Äì1H.)

    st.set_page_config(page_title="Bengali BPE Tokenizer Visualizer", page_icon="üî§", layout="wide")

    st.sidebar.title("üìö Bengali Script Challenges")
    # ... write the same educational content as in the notebook (why NFC, why \X, etc.)

    st.title("üî§ Bengali BPE Tokenizer with Grapheme Analysis")
    samples = {
        "Simple Text": "‡¶Ü‡¶Æ‡¶ø ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶Ø‡¶º ‡¶ï‡¶•‡¶æ ‡¶¨‡¶≤‡¶ø‡•§",
        "Complex Conjuncts": "‡¶ï‡ßç‡¶∑‡ßÅ‡¶¶‡ßç‡¶∞ ‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ ‡¶¨‡ßç‡¶Ø‡¶ï‡ßç‡¶§‡¶ø‡¶§‡ßç‡¶¨ ‡¶∏‡ßç‡¶¨‡¶æ‡¶ß‡ßÄ‡¶®‡¶§‡¶æ",
        "Mixed Diacritics": "‡¶ï‡ßÄ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá? ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º ‡¶ó‡¶ø‡¶Ø‡¶º‡ßá‡¶õ‡¶ø‡¶≤‡ßá?",
        "Long Speech": "‚Ä¶your longer passage‚Ä¶"
    }

    selected = st.selectbox("Choose a sample:", list(samples.keys()))
    user_text = st.text_area("Enter Bengali text:", value=samples[selected], height=150)
    max_merges = st.slider("Number of BPE merges", 1, 15, 8)

    if user_text.strip():
        # Grapheme analysis
        analysis = analyze_bengali_text_structure(user_text)
        # Show grapheme chips, dataframe, and metrics‚Ä¶

        # BPE tokenization
        tokens, merges, history, initial_len, analysis2 = bengali_utf8_bpe_with_grapheme_awareness(user_text, max_merges)

        # Visualizations
        colored_html, token_colors = generate_grapheme_aware_visualization(analysis2, tokens, merges)
        st.markdown(colored_html, unsafe_allow_html=True)

        boundary_html = generate_grapheme_boundary_visualization(analysis2, tokens, merges)
        st.markdown(boundary_html, unsafe_allow_html=True)

        # Metrics: total tokens, violations, compression, etc.

    st.markdown("---")
    st.write("‚úÖ Key insights: preserve graphemes, normalize first, avoid boundary-crossing merges.")

9) LICENSE & CREDITS
--------------------
- You may reuse or adapt this README and the demonstrated approach for research/education.
- Unicode ¬© respective copyright holders. Regex `\X` semantics per Unicode Grapheme Cluster Boundaries.
- Streamlit, Plotly, pandas, numpy are licensed per their respective projects.


END OF DOCUMENT
## Training Data Bias

Even with perfect tokenization, model quality depends on training data:
- Limited Bengali corpora = poorer representation
- Biased datasets = biased outputs
- Grapheme-aware BPE solves **form**, not **content**

---

##  Future Work

- Extend to other Indic scripts (Hindi, Tamil, etc.)
- Train larger corpora with grapheme constraints
- Benchmark improvements on NLP tasks

---


